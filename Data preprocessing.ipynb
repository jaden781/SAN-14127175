{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f937b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30a6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = \"C:\\\\Users\\\\87738\\\\Desktop\\\\ERP\\\\math_anxiety_2020_2024.csv\"\n",
    "df = pd.read_csv(path_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15cdce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"lang\" in df.columns:\n",
    "    df = df[df[\"lang\"].astype(str).str.lower().eq(\"en\")].copy()\n",
    "if \"matches_query\" in df.columns:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d50e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_re = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "hashtag_re = re.compile(r\"#\\w+\")\n",
    "non_ascii_re = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "multi_space_re = re.compile(r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632956f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text\n",
    "    t = url_re.sub(\" \", t)\n",
    "    t = mention_re.sub(\" \", t)\n",
    "    t = hashtag_re.sub(\" \", t)\n",
    "    t = non_ascii_re.sub(\" \", t)  # remove non-ascii (non-English chars)\n",
    "    # remove special symbols except common punctuation\n",
    "    t = re.sub(r\"[^A-Za-z0-9\\.\\,\\!\\?\\'\\-\\s]\", \" \", t)\n",
    "    t = t.lower()\n",
    "    t = multi_space_re.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "text_col = \"text\" if \"text\" in df.columns else df.columns[0]\n",
    "df[\"clean_text\"] = df[text_col].astype(str).map(basic_clean)\n",
    "\n",
    "# Drop rows that became empty after cleaning\n",
    "df = df[df[\"clean_text\"].str.len() > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda9bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_re = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?|\\d+\")\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    return token_re.findall(s)\n",
    "\n",
    "df[\"tokens\"] = df[\"clean_text\"].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec616e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_method = None\n",
    "lemmas = []\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        lemmas_method = \"spacy_lemmatize_en_core_web_sm\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            nlp = spacy.blank(\"en\")\n",
    "            if \"lemmatizer\" not in nlp.pipe_names:\n",
    "                from spacy.lang.en import English\n",
    "                nlp = English()\n",
    "                raise RuntimeError(\"spaCy model not available with lemmatizer\")\n",
    "            lemmas_method = \"spacy_lemmatize_blank_en\"\n",
    "        except Exception:\n",
    "            nlp = None\n",
    "            lemmas_method = None\n",
    "    if nlp:\n",
    "        joined = df[\"tokens\"].map(lambda toks: \" \".join(toks)).tolist()\n",
    "        docs = list(nlp.pipe(joined, batch_size=500))\n",
    "        for doc in docs:\n",
    "            lemmas.append([t.lemma_ if t.lemma_ != \"\" else t.text for t in doc])\n",
    "except Exception:\n",
    "    nlp = None\n",
    "    lemmas_method = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d121947",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lemmas_method is None:\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        def nltk_lemmatize(tokens):\n",
    "            return [lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "        df[\"lemmas\"] = df[\"tokens\"].map(nltk_lemmatize)\n",
    "        lemmas_method = \"nltk_wordnet_lemmatizer\"\n",
    "    except Exception:\n",
    "        stemmer = PorterStemmer()\n",
    "        def porter_stem(tokens):\n",
    "            return [stemmer.stem(tok) for tok in tokens]\n",
    "        df[\"lemmas\"] = df[\"tokens\"].map(porter_stem)\n",
    "        lemmas_method = \"nltk_porter_stemmer_fallback\"\n",
    "if lemmas and \"lemmas\" not in df.columns:\n",
    "    df[\"lemmas\"] = lemmas\n",
    "df[\"processed_text\"] = df[\"lemmas\"].map(lambda toks: \" \".join(toks))\n",
    "\n",
    "out_path = r\"C:\\Users\\87738\\Documents\\math_anxiety_tweets_preprocessed.csv\"\n",
    "df.to_csv(out_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c8bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
